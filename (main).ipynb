{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833680cb",
   "metadata": {},
   "source": [
    "# projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbf675",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6124fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "import py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de39e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Company            Pattern Frequency       Start         End  \\\n",
      "0  AXISCADES  AscendingTriangle     DAILY  1714608000  1724889600   \n",
      "1   BOSCHLTD  AscendingTriangle     DAILY  1722902400  1724889600   \n",
      "2  CELLPOINT  AscendingTriangle     DAILY  1724112000  1724889600   \n",
      "3     CHAVDA  AscendingTriangle     DAILY  1710288000  1724889600   \n",
      "4       DHTL  AscendingTriangle     DAILY  1722988800  1724889600   \n",
      "\n",
      "                                               Array  StartDate    EndDate  \n",
      "0  [652.0374908447266, 663.9124908447266, 653.449... 2024-05-02 2024-08-29  \n",
      "1  [32948.2119140625, 32657.44970703125, 32596.37... 2024-08-06 2024-08-29  \n",
      "2  [31.66249990463257, 31.962499618530273, 32.212... 2024-08-20 2024-08-29  \n",
      "3  [97.39999961853027, 94.17499923706055, 97.3249... 2024-03-13 2024-08-29  \n",
      "4  [90.75, 88.8125, 93.75, 98.39999771118164, 104... 2024-08-07 2024-08-29  \n",
      "['AscendingTriangle' 'AscendingTriangleBO' 'AscendingTriangleBD'\n",
      " 'DescendingTriangle' 'SymmetricTriangle' 'SymmetricTriangleBO'\n",
      " 'AscendingChannelBO' 'AscendingChannelBD' 'DescendingChannel'\n",
      " 'DescendingChannelBD' 'RectangleChannel' 'RectangleChannelBO'\n",
      " 'RectangleChannelBD' 'RectangleChannelPotential' 'DoubleBottom'\n",
      " 'DoubleBottomPotential' 'DoubleBottomInitiate' 'TripleBottomPotential'\n",
      " 'ReverseHeadAndShoulder' 'ReverseHeadAndShoulderPotential'\n",
      " 'RoundingBottom' 'BullishFlag' 'CupAndHandle' 'BullishPennant'\n",
      " 'DoubleTop' 'DoubleTopPotential' 'TripleTop' 'DoubleTopInitiate'\n",
      " 'TripleTopPotential' 'HeadAndShoulder' 'HeadAndShoulderPotential'\n",
      " 'RoundingTop' 'BearishFlag' 'InverseCupAndHandle' 'BearishPennant'\n",
      " 'TripleBottom' 'DescendingTriangleBD']\n",
      "59032\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Hermaeus1618/PatternRecognition\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "# Caminho para o arquivo ZIP\n",
    "ZIP_PATH = \"DATASET.zfs\"\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    raise FileNotFoundError(f\"The file '{ZIP_PATH}' does not exist. Please check the file path.\")\n",
    "\n",
    "# Lista vazia para armazenar os dados\n",
    "data = []\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zfile:\n",
    "    # Ler todos os nomes de arquivos no ZIP\n",
    "    filenames = [f.filename for f in zfile.infolist()]\n",
    "  \n",
    "    for fname in filenames:\n",
    "        # Dividir o nome do arquivo em partes\n",
    "        parts = fname.split(\"_GAP_\")\n",
    "        if len(parts) != 5:\n",
    "            continue  # Ignorar formatos inválidos\n",
    "        \n",
    "        company, pattern, frequency, start, end = parts\n",
    "        \n",
    "        # Ler os dados binários e convertê-los para um array Float64\n",
    "        with zfile.open(fname) as file:\n",
    "            array = np.frombuffer(file.read(), dtype=np.float64)\n",
    "        \n",
    "        # Adicionar os dados à lista\n",
    "        data.append({\n",
    "            \"Company\": company,\n",
    "            \"Pattern\": pattern,\n",
    "            \"Frequency\": frequency,\n",
    "            \"Start\": int(start),\n",
    "            \"End\": int(end),\n",
    "            \"Array\": array\n",
    "        })\n",
    "\n",
    "# Criar um DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df[\"StartDate\"] = pd.to_datetime(df[\"Start\"], unit=\"s\")\n",
    "df[\"EndDate\"] = pd.to_datetime(df[\"End\"], unit=\"s\")\n",
    "\n",
    "# Exibir uma prévia\n",
    "print(df.head())\n",
    "print(df[\"Pattern\"].unique())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7bc859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Company            Pattern Frequency       Start         End  \\\n",
      "0  AXISCADES  AscendingTriangle     DAILY  1714608000  1724889600   \n",
      "1   BOSCHLTD  AscendingTriangle     DAILY  1722902400  1724889600   \n",
      "2  CELLPOINT  AscendingTriangle     DAILY  1724112000  1724889600   \n",
      "3     CHAVDA  AscendingTriangle     DAILY  1710288000  1724889600   \n",
      "4       DHTL  AscendingTriangle     DAILY  1722988800  1724889600   \n",
      "\n",
      "                                               Array  StartDate    EndDate  \n",
      "0  [652.0374908447266, 663.9124908447266, 653.449... 2024-05-02 2024-08-29  \n",
      "1  [32948.2119140625, 32657.44970703125, 32596.37... 2024-08-06 2024-08-29  \n",
      "2  [31.66249990463257, 31.962499618530273, 32.212... 2024-08-20 2024-08-29  \n",
      "3  [97.39999961853027, 94.17499923706055, 97.3249... 2024-03-13 2024-08-29  \n",
      "4  [90.75, 88.8125, 93.75, 98.39999771118164, 104... 2024-08-07 2024-08-29  \n",
      "['AscendingTriangle' 'AscendingTriangleBO' 'AscendingTriangleBD'\n",
      " 'DescendingTriangle' 'SymmetricTriangle' 'SymmetricTriangleBO'\n",
      " 'AscendingChannelBO' 'AscendingChannelBD' 'DescendingChannel'\n",
      " 'DescendingChannelBD' 'RectangleChannel' 'RectangleChannelBO'\n",
      " 'RectangleChannelBD' 'RectangleChannelPotential' 'DoubleBottom'\n",
      " 'DoubleBottomPotential' 'DoubleBottomInitiate' 'TripleBottomPotential'\n",
      " 'ReverseHeadAndShoulder' 'ReverseHeadAndShoulderPotential'\n",
      " 'RoundingBottom' 'BullishFlag' 'CupAndHandle' 'BullishPennant'\n",
      " 'DoubleTop' 'DoubleTopPotential' 'TripleTop' 'DoubleTopInitiate'\n",
      " 'TripleTopPotential' 'HeadAndShoulder' 'HeadAndShoulderPotential'\n",
      " 'RoundingTop' 'BearishFlag' 'InverseCupAndHandle' 'BearishPennant'\n",
      " 'TripleBottom' 'DescendingTriangleBD']\n",
      "59032\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Hermaeus1618/PatternRecognition\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Caminho para o arquivo ZIP\n",
    "ZIP_PATH = \"DATASET.zfs\"\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    raise FileNotFoundError(f\"The file '{ZIP_PATH}' does not exist. Please check the file path.\")\n",
    "\n",
    "# Lista vazia para armazenar os dados\n",
    "data = []\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zfile:\n",
    "    # Ler todos os nomes de arquivos no ZIP\n",
    "    filenames = [f.filename for f in zfile.infolist()]\n",
    "  \n",
    "    for fname in filenames:\n",
    "        # Dividir o nome do arquivo em partes\n",
    "        parts = fname.split(\"_GAP_\")\n",
    "        if len(parts) != 5:\n",
    "            print(f\"Invalid filename format: {fname}\")\n",
    "            continue  # Ignorar formatos inválidos\n",
    "        \n",
    "        try:\n",
    "            # Garantir que as partes sejam atribuídas corretamente\n",
    "            company, pattern, frequency, start, end = parts\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing filename parts: {parts}. Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Ler os dados binários e convertê-los para um array Float64\n",
    "        try:\n",
    "            with zfile.open(fname) as file:\n",
    "                array = np.frombuffer(file.read(), dtype=np.float64)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {fname}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Adicionar os dados à lista\n",
    "        data.append({\n",
    "            \"Company\": company,\n",
    "            \"Pattern\": pattern,\n",
    "            \"Frequency\": frequency,\n",
    "            \"Start\": start,\n",
    "            \"End\": end,\n",
    "            \"Array\": array\n",
    "        })\n",
    "\n",
    "# Criar um DataFrame se houver dados\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Verificar se as colunas 'Start' e 'End' existem antes de convertê-las\n",
    "    if \"Start\" in df.columns and \"End\" in df.columns:\n",
    "        df[\"StartDate\"] = pd.to_datetime(df[\"Start\"], unit=\"s\")\n",
    "        df[\"EndDate\"] = pd.to_datetime(df[\"End\"], unit=\"s\")\n",
    "    else:\n",
    "        print(\"Columns 'Start' or 'End' are missing in the DataFrame. Please check the data.\")\n",
    "        df = pd.DataFrame()  # Criar um DataFrame vazio para evitar erros posteriores\n",
    "else:\n",
    "    print(\"No valid data found. Please check the filenames and data format.\")\n",
    "    df = pd.DataFrame()  # Criar um DataFrame vazio para evitar erros posteriores\n",
    "\n",
    "# Exibir uma prévia se o DataFrame não estiver vazio\n",
    "if not df.empty:\n",
    "    print(df.head())\n",
    "    print(df[\"Pattern\"].unique())\n",
    "    print(len(df))\n",
    "else:\n",
    "    print(\"DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8aeabc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Pattern\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d539116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_save_all_by_pattern(zip_path, pattern_name, max_plots=None):\n",
    "    # Zielordner erstellen\n",
    "    os.makedirs(pattern_name, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zfile:\n",
    "        # Dateinamen und Metadaten extrahieren\n",
    "        files = pd.Series([f.filename for f in zfile.infolist()])\n",
    "        meta_df = files.str.split(\"_GAP_\", expand=True)\n",
    "        meta_df.columns = [\"ticker\", \"pattern\", \"freq\", \"start_ts\", \"end_ts\"]\n",
    "        meta_df[\"filename\"] = files\n",
    "\n",
    "        # Filter auf gewünschtes Pattern\n",
    "        subset = meta_df[meta_df[\"pattern\"] == pattern_name]\n",
    "\n",
    "        # Begrenzung der Anzahl der Plots\n",
    "        if max_plots is not None:\n",
    "            subset = subset.head(max_plots)\n",
    "\n",
    "        for i, row in subset.iterrows():\n",
    "            file = row[\"filename\"]\n",
    "            ticker = row[\"ticker\"]\n",
    "            try:\n",
    "                with zfile.open(file) as f:\n",
    "                    data = np.frombuffer(f.read(), dtype=np.float64)\n",
    "\n",
    "                # Plot erstellen\n",
    "                plt.figure(figsize=(8, 3))\n",
    "                plt.plot(data, color=\"black\")  # Linie in Schwarz\n",
    "                plt.axis(\"off\")  # Achsen ausblenden\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Dateiname sichern\n",
    "                safe_ticker = ticker.replace(\"/\", \"_\")\n",
    "                filename = os.path.join(pattern_name, f\"{i:03d}_{safe_ticker}.png\")\n",
    "                plt.savefig(filename, bbox_inches=\"tight\", pad_inches=0)\n",
    "                plt.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Verarbeitung von {file}: {e}\")\n",
    "\n",
    "# Plots für die ersten 10 Dateien erstellen\n",
    "plot_and_save_all_by_pattern(\"DATASET.zfs\", \"HeadAndShoulder\", max_plots=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a350e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_save_all_by_pattern(zip_path, pattern_name, max_plots=None):\n",
    "    # Criar o diretório de destino\n",
    "    os.makedirs(pattern_name, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zfile:\n",
    "        # Extrair nomes de arquivos e metadados\n",
    "        files = pd.Series([f.filename for f in zfile.infolist()])\n",
    "        meta_df = files.str.split(\"_GAP_\", expand=True)\n",
    "        meta_df.columns = [\"ticker\", \"pattern\", \"freq\", \"start_ts\", \"end_ts\"]\n",
    "        meta_df[\"filename\"] = files\n",
    "\n",
    "        # Filtrar pelo padrão desejado\n",
    "        subset = meta_df[meta_df[\"pattern\"] == pattern_name]\n",
    "\n",
    "        # Limitar o número de plots\n",
    "        if max_plots is not None:\n",
    "            subset = subset.head(max_plots)\n",
    "\n",
    "        for i, row in subset.iterrows():\n",
    "            file = row[\"filename\"]\n",
    "            ticker = row[\"ticker\"]\n",
    "            try:\n",
    "                with zfile.open(file) as f:\n",
    "                    data = np.frombuffer(f.read(), dtype=np.float64)\n",
    "\n",
    "                # Criar o plot\n",
    "                plt.figure(figsize=(8, 3))\n",
    "                plt.plot(data)\n",
    "                #plt.title(f\"{ticker} ({pattern_name})\")\n",
    "                plt.xlabel(\"Index\")\n",
    "                plt.ylabel(\"Valor\")\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Salvar o arquivo com um nome seguro\n",
    "                safe_ticker = ticker.replace(\"/\", \"_\")\n",
    "                filename = os.path.join(pattern_name, f\"{i:03d}_{safe_ticker}.png\")\n",
    "                plt.savefig(filename)\n",
    "                plt.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {file}: {e}\")\n",
    "\n",
    "# Criar os plots para os primeiros 10 arquivos\n",
    "plot_and_save_all_by_pattern(\"DATASET.zfs\", \"HeadAndShoulder\", max_plots=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba3331f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Start'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14660\\2182717472.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;31m# Ensure the index is a DatetimeIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Filter rows where the \"Array\" column has exactly 4 elements (OHLC data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\dhbw_s3\\wds23s4_fibu_chartformation\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   6118\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['Start'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Ensure the index is a DatetimeIndex\n",
    "df = df.set_index(\"Start\")\n",
    "\n",
    "# Filter rows where the \"Array\" column has exactly 4 elements (OHLC data)\n",
    "df = df[df[\"Array\"].apply(lambda x: len(x) == 4)]\n",
    "\n",
    "# Extract the required columns for candlestick plotting\n",
    "# Assuming the \"Array\" column contains OHLC data in the order [Open, High, Low, Close]\n",
    "ohlc_data = pd.DataFrame(df[\"Array\"].tolist(), index=df.index, columns=[\"Open\", \"High\", \"Low\", \"Close\"])\n",
    "\n",
    "# Plot the data\n",
    "mpf.plot(\n",
    "    ohlc_data[-50:], \n",
    "    type='candle', \n",
    "    style='charles', \n",
    "    figsize=(3, 3),\n",
    "    axisoff=True,\n",
    "    savefig='images/DoubleTop/sample_001.png'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
